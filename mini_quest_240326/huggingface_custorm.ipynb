{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3476d2e",
   "metadata": {},
   "source": [
    "# STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성\n",
    "- 데이터셋은 깃허브에서 다운받거나, [Huggingface datasets](https://huggingface.co/datasets)에서 가져올 수 있습니다. 앞에서 배운 방법들을 활용해봅시다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97a45371",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796bd8c22055465f997d6a033ed9f4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3996088ade154a5cbb1060f94456d25c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset nsmc/default to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7581d80654c47c585e170d5f44edb3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919085fcbe7549fd9671cde505025fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f8ede4900544cea67aaee60319bf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/2.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684f8ec7d84e4950a300961cfbec9b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset nsmc downloaded and prepared to /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n",
      "Found cached dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "train_data = load_dataset('nsmc', split='train[:80%]') #30\n",
    "val_data = load_dataset('nsmc',split='train[-7%:]') #-5\n",
    "test_data = load_dataset('nsmc',split='test[:21%]') #15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f448b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 120000\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa5ed8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 10500\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4655b415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'document', 'label'],\n",
       "    num_rows: 10500\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1b5e1",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59c68ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# train_df = pd.DataFrame(train_data, columns=['id', 'document', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d243be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['document'].nunique(), train_df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d97fef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.drop_duplicates(subset=['document'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61c82ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('총 샘플의 수 :',len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0142c0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209d8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df.groupby('label').size().reset_index(name='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41341551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf5659b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 한글과 공백을 제외하고 모두 제거\n",
    "# train_df['document'] = train_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "# train_df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73ede356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # 공백만 있는 값들을 Null로 변경\n",
    "# train_df['document'] = train_df['document'].str.replace('^ +', \"\") # white space 데이터를 empty value로 변경\n",
    "# train_df['document'].replace('', np.nan, inplace=True)\n",
    "# print(train_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ed3912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.loc[train_df.document.isnull()][:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2125eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df = pd.DataFrame(test_data, columns=['id', 'document', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a05dfda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "# test_df['document'] = test_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "# test_df['document'] = test_df['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "# test_df['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "# test_df = test_df.dropna(how='any') # Null 값 제거\n",
    "# print('전처리 후 테스트용 샘플의 개수 :',len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d65f4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.DataFrame(val_data, columns=['id', 'document', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52942a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.drop_duplicates(subset = ['document'], inplace=True) # document 열에서 중복인 내용이 있다면 중복 제거\n",
    "# val_df['document'] = val_df['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "# val_df['document'] = val_df['document'].str.replace('^ +', \"\") # 공백은 empty 값으로 변경\n",
    "# val_df['document'].replace('', np.nan, inplace=True) # 공백은 Null 값으로 변경\n",
    "# val_df = val_df.dropna(how='any') # Null 값 제거\n",
    "# print('전처리 후 테스트용 샘플의 개수 :',len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb887814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = Dataset.from_pandas(train_df)\n",
    "\n",
    "# # 변환 함수 적용\n",
    "# # transformed_dataset = train_dataset.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56b15eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6a0e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = huggingface_nsmc_dataset['train']\n",
    "# cols = train.column_names\n",
    "# cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91e46ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i in range(5):\n",
    "#     for col in cols:\n",
    "#         print(col, ':', train[col][i])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc9f087",
   "metadata": {},
   "source": [
    "nsmc에는 train, 과 test로만 구성된것을 확인\n",
    "그리고 데이터 속 colum들도 확인한 결과 id, document, label 로 구성된것을 확인했다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b045c2b",
   "metadata": {},
   "source": [
    "# STEP 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00b0c21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", use_fast=True)\n",
    "\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-small\", num_labels=2)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "872739c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return tokenizer(\n",
    "        data['document'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=30,\n",
    "        return_token_type_ids=False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4cf0778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-3b0438473d9d52d4.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-7c8bf3841eee4408.arrow\n",
      "Loading cached processed dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-2bfefb7a5d16dca8.arrow\n"
     ]
    }
   ],
   "source": [
    "train_data = train_data.map(transform, batched=True)\n",
    "test_data = test_data.map(transform, batched=True)\n",
    "val_data = val_data.map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b92127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378b03cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3455b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "752221b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_dataset = huggingface_nsmc_dataset.map(transform, batched=True)\n",
    "\n",
    "# # train & validation & test split\n",
    "# # import random\n",
    "\n",
    "# # 무작위로 90000개의 데이터만 선택\n",
    "# hf_train_dataset = hf_dataset['train'][:]\n",
    "# # hf_train_dataset = hf_train_dataset.select(indices)\n",
    "# hf_test_dataset = hf_dataset['test'][:25000]\n",
    "# hf_val_dataset = hf_dataset['test'][25000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f626fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16f414ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hf_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b33afe",
   "metadata": {},
   "source": [
    "# STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3fd7356f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = os.getenv(\"HOME\") + '/aiffel/aiffel_quest/mini_quest_240326'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "output_dir,                        # output이 저장될 경로\n",
    "evaluation_strategy='epoch',       # evaluation하는 빈도\n",
    "learning_rate=2e-5,                # learning_rate\n",
    "per_device_train_batch_size=576,     # 각 device 당 batch size(8)\n",
    "per_device_eval_batch_size=576,      # evaluation 시에 batch size(8)\n",
    "num_train_epochs=8,                # train 시킬 총 epochs\n",
    "weight_decay=0.01)                 # weight decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b33e8dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d25a8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 코드 실행 중간에 GPU 메모리를 해제해야 하는 경우\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383c7ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 576\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 576\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1672\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1672' max='1672' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1672/1672 1:14:34, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.264576</td>\n",
       "      <td>0.885143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.248351</td>\n",
       "      <td>0.895619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>0.249185</td>\n",
       "      <td>0.896762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.279400</td>\n",
       "      <td>0.253465</td>\n",
       "      <td>0.900286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.266606</td>\n",
       "      <td>0.898286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.278067</td>\n",
       "      <td>0.898286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.182400</td>\n",
       "      <td>0.288220</td>\n",
       "      <td>0.898190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.133700</td>\n",
       "      <td>0.293996</td>\n",
       "      <td>0.897524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 6min 6s, sys: 8min 37s, total: 1h 14min 43s\n",
      "Wall time: 1h 14min 42s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1672, training_loss=0.19038934570750551, metrics={'train_runtime': 4477.434, 'train_samples_per_second': 214.409, 'train_steps_per_second': 0.373, 'total_flos': 1.4799996864e+16, 'train_loss': 0.19038934570750551, 'epoch': 8.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics,)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3de4af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 576\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.32934555411338806,\n",
       " 'eval_accuracy': 0.8917142857142857,\n",
       " 'eval_runtime': 18.3454,\n",
       " 'eval_samples_per_second': 572.351,\n",
       " 'eval_steps_per_second': 1.036,\n",
       " 'epoch': 8.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907af962",
   "metadata": {},
   "source": [
    "# STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기\n",
    "- 데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ff96285f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del training_arguments, trainer, model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff7b61b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", use_fast=True)\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-small\", num_labels=2)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e702835",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.getenv(\"HOME\") + '/aiffel/aiffel_quest/mini_quest_240326'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "output_dir,                        # output이 저장될 경로\n",
    "evaluation_strategy='epoch',       # evaluation하는 빈도\n",
    "learning_rate=2e-5,                # learning_rate\n",
    "per_device_train_batch_size=512,     # 각 device 당 batch size(8)\n",
    "per_device_eval_batch_size=512,      # evaluation 시에 batch size(8)\n",
    "num_train_epochs=10,                # train 시킬 총 epochs\n",
    "weight_decay=0.01)                 # weight decay\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "acdc3146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 512\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2350\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2350' max='2350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2350/2350 1:30:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.259228</td>\n",
       "      <td>0.888095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.247204</td>\n",
       "      <td>0.895143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.247491</td>\n",
       "      <td>0.896952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.283400</td>\n",
       "      <td>0.256262</td>\n",
       "      <td>0.900476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.268784</td>\n",
       "      <td>0.900762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.185500</td>\n",
       "      <td>0.289511</td>\n",
       "      <td>0.899238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.301005</td>\n",
       "      <td>0.899619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.325092</td>\n",
       "      <td>0.898381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.337379</td>\n",
       "      <td>0.898667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.098900</td>\n",
       "      <td>0.342531</td>\n",
       "      <td>0.898381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-1500/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "Saving model checkpoint to /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/aiffel_quest/mini_quest_240326/checkpoint-2000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 20min 16s, sys: 10min 1s, total: 1h 30min 18s\n",
      "Wall time: 1h 30min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2350, training_loss=0.16093394178025267, metrics={'train_runtime': 5455.8327, 'train_samples_per_second': 219.948, 'train_steps_per_second': 0.431, 'total_flos': 1.849999608e+16, 'train_loss': 0.16093394178025267, 'epoch': 10.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics,)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0d4772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10500\n",
      "  Batch size = 512\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3852943778038025,\n",
       " 'eval_accuracy': 0.8865714285714286,\n",
       " 'eval_runtime': 17.0778,\n",
       " 'eval_samples_per_second': 614.834,\n",
       " 'eval_steps_per_second': 1.23,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea572195",
   "metadata": {},
   "source": [
    "# STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교\n",
    "- 아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.\n",
    "    - [Data Collator](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/data_collator)\n",
    "    - [Trainer.TrainingArguments 의 group_by_length](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)\n",
    "- STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "804c75f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del training_arguments, trainer, model, tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f4e2c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9d8661011a04cf4a27cce598ec4e829",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/445M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# 토크나이저와 데이터 콜레이터 정의\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"klue/bert-base\", num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\", use_fast=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "050e9be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def transform(data):\n",
    "    encoded_data = tokenizer(\n",
    "        data['document'],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=30,\n",
    "        return_tensors=\"pt\",\n",
    "        return_token_type_ids=False,)\n",
    "    \n",
    "    # 텐서를 리스트로 변환하여 반환\n",
    "    return {key: value.squeeze().tolist() for key, value in encoded_data.items()}\n",
    "\n",
    "\n",
    "train_data = train_data.map(transform)#, batched=True)\n",
    "test_data = test_data.map(transform)#, batched=True)\n",
    "val_data = val_data.map(transform)#, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1604480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ae12c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getenv(\"HOME\") + '/aiffel/aiffel_quest/mini_quest_240326'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "output_dir,                        # output이 저장될 경로\n",
    "evaluation_strategy='epoch',       # evaluation하는 빈도\n",
    "learning_rate=2e-5,                # learning_rate\n",
    "per_device_train_batch_size=512,     # 각 device 당 batch size(8)\n",
    "per_device_eval_batch_size=512,      # evaluation 시에 batch size(8)\n",
    "num_train_epochs=10,                # train 시킬 총 epochs\n",
    "group_by_length=True,\n",
    "weight_decay=0.1)                 # weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fce5bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9a5308f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2350' max='2350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2350/2350 1:29:07, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.263432</td>\n",
       "      <td>0.887905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.249843</td>\n",
       "      <td>0.896286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.254918</td>\n",
       "      <td>0.895238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.274400</td>\n",
       "      <td>0.260062</td>\n",
       "      <td>0.900095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.273178</td>\n",
       "      <td>0.900286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.184700</td>\n",
       "      <td>0.296920</td>\n",
       "      <td>0.899333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.310607</td>\n",
       "      <td>0.900476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.130300</td>\n",
       "      <td>0.343896</td>\n",
       "      <td>0.896000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.352533</td>\n",
       "      <td>0.898571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.096700</td>\n",
       "      <td>0.358275</td>\n",
       "      <td>0.897619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 20min 50s, sys: 8min 33s, total: 1h 29min 23s\n",
      "Wall time: 1h 29min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2350, training_loss=0.15802060553368102, metrics={'train_runtime': 5349.695, 'train_samples_per_second': 224.312, 'train_steps_per_second': 0.439, 'total_flos': 1.849999608e+16, 'train_loss': 0.15802060553368102, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    data_collator = data_collator,\n",
    "    eval_dataset=val_data,\n",
    "    compute_metrics=compute_metrics,)\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b65331f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3893945515155792,\n",
       " 'eval_accuracy': 0.8895238095238095,\n",
       " 'eval_runtime': 16.8408,\n",
       " 'eval_samples_per_second': 623.484,\n",
       " 'eval_steps_per_second': 1.247,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6064dd",
   "metadata": {},
   "source": [
    "Bucketing 이란?<br>\n",
    "NLP에서의 Bucketing은 데이터를 효율적으로 처리하기 위한 전략 중 하나입니다. 특히, Sequence-to-Sequence 모델을 학습할 때 사용되며, 입력 문장의 길이에 따라 데이터를 그룹화하여 배치를 구성하는 방법입니다.\n",
    "\n",
    "일반적으로, 서로 다른 길이의 문장을 동일한 배치에 함께 처리할 때 모든 문장의 길이를 동일하게 맞추는 것이 일반적입니다. 그러나 이는 GPU의 메모리를 비효율적으로 사용할 수 있습니다. 예를 들어, 모든 배치에 포함된 문장의 최대 길이보다 더 긴 문장이 있을 경우, 모든 문장을 최대 길이에 맞추어 패딩(padding)을 추가해야 합니다. 이렇게 되면 패딩된 토큰들은 실제 의미를 가지지 않지만 연산에도 참여하게 되어 메모리와 계산 시간을 낭비하게 됩니다.\n",
    "\n",
    "Bucketing은 이러한 문제를 해결하기 위한 방법 중 하나입니다. Bucketing은 데이터를 여러 그룹으로 나누고, 각 그룹 내에서는 비슷한 길이의 문장들을 함께 배치로 만듭니다. 이렇게 하면 패딩의 양을 최소화하고 효율적으로 GPU 메모리를 사용할 수 있습니다.\n",
    "\n",
    "예를 들어, Bucketing을 사용하여 문장의 길이에 따라 다음과 같이 데이터를 그룹화할 수 있습니다:\n",
    "\n",
    "길이가 1-10 토큰인 문장들의 그룹\n",
    "길이가 11-20 토큰인 문장들의 그룹\n",
    "길이가 21-30 토큰인 문장들의 그룹\n",
    "...\n",
    "그런 다음 각 그룹에서는 비슷한 길이의 문장들을 배치로 만들어 처리합니다. 이렇게 함으로써 효율적인 학습이 가능해집니다.\n",
    "\n",
    "Bucketing은 특히 Transformer와 같은 Self-Attention 기반의 모델을 학습할 때 유용합니다. Transformer 모델의 Self-Attention 계산은 시퀀스 길이에 따라 계산량이 크게 달라지기 때문에, Bucketing을 통해 비슷한 길이의 문장들을 함께 처리하여 계산 효율을 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b2d57",
   "metadata": {},
   "source": [
    "Dynamic padding이란?<br>\n",
    "Dynamic padding은 패딩을 적용할 때, 각 배치에 포함된 샘플들의 길이에 맞게 패딩을 조절하는 기법입니다. 정적 패딩(static padding)과는 달리, 각 배치마다 서로 다른 길이의 샘플을 가지고 있을 때, 최대한 적은 양의 패딩을 사용하여 효율적으로 메모리를 사용합니다.\n",
    "\n",
    "일반적으로 정적 패딩은 모든 샘플을 특정 길이로 맞추기 위해 최대 길이에 맞게 패딩을 적용하는 반면, 동적 패딩은 각 배치에 포함된 샘플 중 가장 긴 샘플의 길이에 맞게 패딩을 적용합니다. 이렇게 하면 모든 샘플이 동일한 길이로 패딩되는 것이 아니라, 각 샘플의 실제 길이에 맞게 패딩이 적용되어 메모리를 더 효율적으로 사용할 수 있습니다.\n",
    "\n",
    "동적 패딩은 주로 RNN(Recurrent Neural Network)이나 Transformer와 같은 시퀀스 모델에서 사용됩니다. 이러한 모델들은 입력 시퀀스의 길이가 다를 수 있으며, 동적 패딩은 이러한 모델에서 효율적인 배치 처리를 위해 중요한 역할을 합니다. Hugging Face의 Transformers 라이브러리에서도 동적 패딩을 쉽게 적용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7a4042",
   "metadata": {},
   "source": [
    "# 회고\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
