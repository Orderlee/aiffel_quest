{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8921e15",
   "metadata": {},
   "source": [
    "## 데이터셋 확인 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "402de737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aiffel/aiffel/NLP/KoChatGPT/colossalai_ChatGPT_230319\n"
     ]
    }
   ],
   "source": [
    "cd /aiffel/aiffel/NLP/KoChatGPT/colossalai_ChatGPT_230319 # 경로 문제를 해결하기 위해 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58178b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pillow==8.3.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b23299c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\" # skt/ko-gpt-trinity-1.2B-v0.5로 found model 변경 # skt/kogpt2-base-v2\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84e71eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model quantizer\n",
    "# model_name = \"skt/ko-gpt-trinity-1.2B-v0.5\"\n",
    "\n",
    "# # load and preprocess data\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\"     # Allow batched inference\n",
    "# train_data, val_data = load_and_preprocess_data(train_args, tokenizer)\n",
    "\n",
    "# # load model and finetune\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     train_args.base_model,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map={\"\":0},\n",
    "# )\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=train_args.lora_r,\n",
    "#     lora_alpha=train_args.lora_alpha,\n",
    "#     target_modules=train_args.lora_target_modules,\n",
    "#     lora_dropout=train_args.lora_dropout,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fcf7dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2077af6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "data_path_1_SFT = '/aiffel/aiffel/NLP/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "with open(data_path_1_SFT, 'r', encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict_1 = json.load(json_file)\n",
    "     # 토큰을 따로 뽑아서 평균값 보는 작업\n",
    "print(len(list_data_dict_1))\n",
    "list_data_dict_1[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3dee2bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAm0AAAFNCAYAAABST1gVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABHHklEQVR4nO3deXwV1f3/8ddHQEBBA4IWCAoqKHuEsNUN1EJwAUUUhCpULW5YrP21qFBFBL9WrQu21iJQ0LLI4oJWFFTcldWwihIESwBlEwQRC/j5/TGTcEnuTQLkJrnJ+/l43EdmzmfmzJnJlXw8Z86MuTsiIiIiUrIdVdwNEBEREZH8KWkTERERSQBK2kREREQSgJI2ERERkQSgpE1EREQkAShpExEREUkAStpEShEze8bM/lxIdZ1sZrvMrFy4/q6Z3VgYdYf1zTSzvoVV3yEcd7iZbTGzb4r62BKdmQ01s38XdztESjolbSIJwszWmtmPZrbTzLab2cdmdrOZZf937O43u/sDBazrory2cff/unsVd99fCG3P9UfZ3bu4+/gjrfsQ23Ey8Aegsbv/IsY295jZmjBhzTSzFwrp2IWa9BYGM+tgZpml/ZgipYWSNpHEcpm7VwVOAR4CBgFjCvsgZla+sOssIU4Gtrr7pmjBsOfvWuAid68CpAJvF2H7RERiUtImkoDcfYe7zwB6An3NrCmAmY0zs+Hhcg0zey3sldtmZh+Y2VFm9jxB8vJq2Jv0JzOrZ2ZuZjeY2X+BdyLKIhO408xsnpl9b2avmFn18Fi5ek+yevPMLA24B+gZHm9xGM/ueQrbNcTMvjazTWb2nJkdH8ay2tHXzP4bDm0OjnVtzOz4cP/NYX1DwvovAmYDtcN2jIuye2vgTXdfHV7nb9x9VI66x5jZRjNbHw61Zg0f9zOzD83sUTP7Luyt6xLGRgDnAn8Lj/23sPxMM5sd/n6+MLOrI441zsz+bmb/CXtX55rZaRHxJhH7fmtm90Rcy7vMbLWZbTWzKVm/p0NhZrXNbHp4HdeY2e8iYkPDep8L27bczFIj4i3N7LMwNtXMXgiv1bHAzIjfwS4zqx3udnQe9Q0Kr/fO8DpdeKjnI1IaKGkTSWDuPg/IJEgIcvpDGKsJnESQOLm7Xwv8l6DXroq7Pxyxz/lAI6BzjENeB1wP1AL2ASML0MY3gAeBF8LjtYiyWb/w0xE4FagC/C3HNucAZwAXAveaWaMYh3wKOD6s5/ywzb9x97eALsCGsB39ouz7KXCdmf3RzFKzErII4wjO+3TgLKATEDnk2Rb4AqgBPAyMMTNz98HAB8CA8NgDwgRmNjAROBHoBTxtZo0j6usF3A9UAzKAEQBmVhV4C3gDqB22J6tH8Hbg8vDcawPfAX+Pca2ismDI/VVgMVCH4JrfYWaR34uuwGQgCZhB+Psys6OBl8JrVR2YBFwB4O4/cPDvoIq7b8invjOAAUDrsJe5M7D2UM5HpLRQ0iaS+DYQ/HHMaS9BcnWKu+919w88/5cND3X3H9z9xxjx5919WfjH98/A1VESm8PRB3jM3b9y913A3UCvHL1897v7j+6+mCCZyJX8hW3pBdzt7jvdfS3wV4Ihz3y5+78Jkp7OwHvAJjMbFNZ9EnAxcEd4jTYBj4fHy/K1uz8b3gc4nuD6nxTjcJcCa939X+6+z90/A6YDV0Vs85K7z3P3fcAEICVi32/c/a/uvic817lh7GZgsLtnuvtPwFCghx3akHdroKa7D3P3/7n7V8CzOc71Q3d/PTzX5znw+2gHlAdGht+7F4F5BThmrPr2AxWBxmZWwd3XZvWEipQ1pfW+FZGypA6wLUr5IwR/sGeZGcAod38on7rWHUL8a6ACQa/Skaod1hdZd3kOTngiZ3vuJuiNy6lG2KacddUpaEPcfQIwwcwqEPRYTTCzdIIeqwrAxvB6QvA/vpHX5JuIenaH20VrJwT3JbY1s+0RZeUJEpZc9XHwOdcFYiUupwAvmdnPEWX7Ca7l+hj7RKujdo62lSPoLYzVtkphYlgbWJ/jfxDy+17FrM/dM8zsDoLvchMzexO4M6KHTqTMUE+bSAIzs9YECcmHOWNh78sf3P1UgqGnOyPuBYrV45ZfT1zdiOWTCXrztgA/AMdEtKscwbBsQevdQJAoRNa9D/g2n/1y2hK2KWddBU1WsoW9RFOBJUBTgsTjJ6CGuyeFn+PcvUlBq8yxvg54L6KupHC48JYC1LWOYPg3VqxLjnorufuhXIN1wJocdVR194sLsO9GoI5FZLYc/L3J77uQi7tPdPdzCH6vDvzlUOsQKQ2UtIkkIDM7zswuJbgH6N/uvjTKNpea2enhH88dBL0tWb0v3xL7j35efm1mjc3sGGAYMC0czvqSoGfkkrCHagjBkFaWb4F6FvF4khwmAb83s/pmVoUD98DtO5TGhW2ZAowws6pmdgpwJ1CgZ4CFkwkuCfc9KpxI0ASY6+4bgVnAX8Prf5SZnWZm5xeweTmv+WtAQzO71swqhJ/WedyrF+k1oJaZ3WFmFcP2tg1jz4Tnf0p4TjXNrFs+510p8kMwnLkznABQ2czKmVnT8H8S8vMJwXdtgJmVD4/dJsd1OMHCiSb5MbMzzOwCM6sI7AF+5MD3WKRMUdImklheNbOdBD0hg4HHgN/E2LYBwc3quwj+kD7t7nPC2P8BQyyYWfr/DuH4zxPcYP4NUAn4HQSzWYFbgdEEvVo/EEyCyDI1/LnVzBZFqXdsWPf7wBqCP863H0K7It0eHv8rgh7IiWH9BfE9wYSN/wLbCSYT3OLuWT2Z1wFHAysIhkunEdy3VhBPEtxb9p2ZjXT3nQQTGXoR9DR+Q9CDVDGPOoCgFxX4FXBZuN8qgkkcWceZQTAsvpNgckXbaPWE6hAkQpGf+gT3zaUQ/D62EPxu80203P1/QHfgBoJr+GuCJPOnML6SIEn/Kvz+1Y5RVZaKBI+32RKe64kE9zyKlDmW/33JIiIih8/M5gLPuPu/irstIolMPW0iIlKozOx8M/tFODzaF2hO8HgSETkCmj0qIiKF7QyCewuPJRim7hHeEygiR0DDoyIiIiIJQMOjIiIiIglASZuIiIhIAiiV97TVqFHD69WrV9zNKDYLNywEoFXtVlGCQYxWUWIiIiJS5BYuXLjF3Wvmt12pvKctNTXVFyxYUNzNKDZ2f/Agcr8vyu826yHlpfD3LiIikojMbKG7p+a3nYZHRURERBKAkjYRERGRBKCkTURERCQBlMqJCCIiIodi7969ZGZmsmfPnuJuipRilSpVIjk5mQoVKhzW/kraRESkzMvMzKRq1arUq1cPy5qwJVKI3J2tW7eSmZlJ/fr1D6uOuA+Pmlk5M/vMzF4L1+ub2VwzyzCzF8zs6LC8YrieEcbrRdRxd1j+hZl1jnebE53f59FnjkIwa1QzR0VEDrJnzx5OOOEEJWwSN2bGCSeccES9uUVxT9tA4POI9b8Aj7v76cB3wA1h+Q3Ad2H54+F2mFljoBfQBEgDnjazckXQbhERKUOUsEm8Hel3LK5Jm5klA5cAo8N1Ay4ApoWbjAcuD5e7heuE8QvD7bsBk939J3dfA2QAbeLZbhERkeLw8ssvY2asXLmyuJuSr3r16rFly5a41T9u3Dg2bNhwRMe7/vrrOfHEE2natGnU+F//+lfMLFe98+fPp3z58kybNi3qfoMHD6Zu3bpUqVLloPL333+fli1b5rnvkYh3T9sTwJ+An8P1E4Dt7r4vXM8E6oTLdYB1AGF8R7h9dnmUfSSKVqNa0WpUjDcetGqltyGIiJRQkyZN4pxzzmHSpEmFUt/+/fsLpZ7ikDNpOxz9+vXjjTfeiBpbt24ds2bN4uSTTz6ofP/+/QwaNIhOnTrFrPeyyy5j3rx5ucpPPvlkxo0bR+/evY+o3bHELWkzs0uBTe6+MF7HyHG8/ma2wMwWbN68uSgOWWIt2riIRRsXxQguCj4iIlKi7Nq1iw8//JAxY8YwefJkAN544w2uuuqq7G3effddLr30UgBmzZpF+/btadmyJVdddRW7du0Cgh6pQYMG0bJlS6ZOncqzzz5L69atadGiBVdeeSW7d+8GYPXq1bRr145mzZoxZMiQg3qNHnnkEVq3bk3z5s257777CnwOmzdv5sorr6R169a0bt2ajz76CIChQ4dy/fXX06FDB0499VRGjhyZvc8DDzzAGWecwTnnnMM111zDo48+yrRp01iwYAF9+vQhJSWFH3/8EYCnnnqKli1b0qxZswL1Rp533nlUr149auz3v/89Dz/8cK4hy6eeeoorr7ySE088MWa97dq1o1atWrnK69WrR/PmzTnqqPikV/HsaTsb6Gpma4HJBMOiTwJJZpY1azUZWB8urwfqAoTx44GtkeVR9snm7qPcPdXdU2vWzPf1XSIiIiXKK6+8QlpaGg0bNuSEE05g4cKFXHTRRcydO5cffvgBgBdeeIFevXqxZcsWhg8fzltvvcWiRYtITU3lsccey67rhBNOYNGiRfTq1Yvu3bszf/58Fi9eTKNGjRgzZgwAAwcOZODAgSxdupTk5OTsfWfNmsWqVauYN28e6enpLFy4kPfff79A5zBw4EB+//vfM3/+fKZPn86NN96YHVu5ciVvvvkm8+bN4/7772fv3r3Z2y1evJiZM2eS9QrKHj16kJqayoQJE0hPT6dy5coA1KhRg0WLFnHLLbfw6KOPArBgwYKDjlPQa12nTh1atGhxUPn69et56aWXuOWWWw6pvqISt6TN3e9292R3r0cwkeAdd+8DzAF6hJv1BV4Jl2eE64Txdzx4MeoMoFc4u7Q+0ADI3SdZDOqeEkwNj+fn6EqVD3mfLIcSq3tKvWK4giIiJZRZ7M+oUQe2GzUq720PwaRJk+jVqxcAvXr1YtKkSZQvX560tDReffVV9u3bx3/+8x+6devGp59+yooVKzj77LNJSUlh/PjxfP3119l19ezZM3t52bJlnHvuuTRr1owJEyawfPlyAD755JPsXrzI4bxZs2Yxa9YszjrrLFq2bMnKlStZtWpVgc7hrbfeYsCAAaSkpNC1a1e+//777B7ASy65hIoVK1KjRg1OPPFEvv32Wz766CO6detGpUqVqFq1Kpdddlme9Xfv3h2AVq1asXbtWgBSU1MZPXp0gdoHsHv3bh588EGGDRuWK3bHHXfwl7/8JW49ZUeqOJ7TNgiYbGbDgc+AMWH5GOB5M8sAthEkerj7cjObAqwA9gG3uXuJGKTP/O/XTPhye1yP0adh0iEfo8/EJIDo+zWMHusTlouISNHbtm0b77zzDkuXLsXM2L9/P2bGI488Qq9evfjb3/5G9erVSU1NpWrVqrg7v/rVr2Le+3bsscdmL/fr14+XX36ZFi1aMG7cON5999082+Lu3H333dx0002HfB4///wzn376KZUqVcoVq1ixYvZyuXLl2LdvX65t8pNVx+HuD8Gw8Jo1a7J72TIzM2nZsiXz5s1jwYIF2Ynzli1beP311ylfvjyXX375YR2rsBVJKunu77r7peHyV+7ext1Pd/er3P2nsHxPuH56GP8qYv8R7n6au5/h7jOLos0iIlKGZT3TMtqnf/8D2/Xvn/e2BTRt2jSuvfZavv76a9auXcu6deuoX78+H3zwAeeffz6LFi3i2WefzU4o2rVrx0cffURGRgYAP/zwA19++WXUunfu3EmtWrXYu3cvEyZMyC5v164d06dPB8i+hw6gc+fOjB07NruHbP369WzatKlA59GpUyeeeuqp7PX09PQ8tz/77LN59dVX2bNnD7t27eK1117LjlWtWpWdO3cW6LiHolmzZmzatIm1a9eydu1akpOTWbRoEb/4xS9Ys2ZNdnmPHj14+umnS0zCBnr3qIiISLGbNGkSV1xxxUFlV155JZMmTaJcuXJceumlzJw5M3sSQs2aNRk3bhzXXHMNzZs3p3379jFvzH/ggQdo27YtZ599NmeeeWZ2+RNPPMFjjz1G8+bNycjI4PjjjweCxKt37960b9+eZs2a0aNHj5jJU/PmzUlOTiY5OZk777yTkSNHsmDBApo3b07jxo155pln8jzv1q1b07VrV5o3b06XLl1o1qxZdjv69evHzTfffNBEhGjyuqftmmuuoX379nzxxRckJydn3893OFJSUrKX//SnP5GcnMzu3btJTk5m6NChQPCokOTkZKZOncpNN91EkyZNDvt40ZiXwqfjp6ametbNjPFkZiVyeHT03IEA3Nj2yVyxNkOC2LzhB8f6NEyiNH4XREQK4vPPP6dRo0bF3YwitXv3bipXDu6bnjx5MpMmTeKVV17Jf8dCtmvXLqpUqcLu3bs577zzGDVqFC1btizydhSVaN81M1vo7qn57at3j5ZC0ZK1LDmTNRERKZsWLlzIgAEDcHeSkpIYO3ZssbSjf//+rFixgj179tC3b99SnbAdKSVtIiIiZdC5557L4sWLi7sZTJw4sbibkDB0T1sptGZbOmu2pUeNVVuWTrVl0WMiIiJScqmnrRQa8kYHACb03p4r1qV7EJsY53vxREREpHCpp01EREQkAShpExEREUkAStpERERKgBEjRtCkSROaN29OSkoKc+fOBYLnqWW95D2ncePGMWDAgKjlNWvW5KyzzqJBgwZ07tyZjz/++LDbdu+99/LWW2/luc24cePYsGFD9vqNN97IihUrDvuYsaSlpZGUlJT9zLosffr04YwzzqBp06Zcf/317N27F4AJEybQvHlzmjVrxi9/+cuYky/efvttWrZsSUpKCuecc072g4uzTJ8+HTOjKB4pFouSNhERkWL2ySef8Nprr7Fo0SKWLFnCW2+9Rd26dYG8k7a89OzZk88++4xVq1Zx11130b17dz7//PPDat+wYcO46KKL8twmZ9I2evRoGjdufFjHy8sf//hHnn/++Vzlffr0YeXKlSxdupQff/wx+32k9evX57333mPp0qX8+c9/pn/kGy0i3HLLLdkvqO/duzfDhw/Pju3cuZMnn3yStm3bFvr5HAolbSIiIsVs48aN1KhRI/vdmjVq1KB27dqMHDmSDRs20LFjRzp27AjAv/71Lxo2bEibNm346KOPClR/x44d6d+/P6PCl92vXr2atLQ0WrVqxbnnnsvKlSvZsWMHp5xyCj///DMQvBqrbt267N27l379+jFt2jQgSOBat25N06ZN6d+/P+7OtGnTWLBgAX369Ml+g0GHDh2ye6UmTZpEs2bNaNq0KYMGDcpuV5UqVRg8eDAtWrSgXbt2fPvtt/mey4UXXkjVqlVzlV988cWYGWZGmzZtyMzMBOCXv/wl1apVA4JXd2WV52RmfP/99wDs2LGD2rVrZ8f+/Oc/M2jQoKjvVC1KStpERESKWadOnVi3bh0NGzbk1ltv5b333gPgd7/7HbVr12bOnDnMmTOHjRs3ct999/HRRx/x4YcfHtLwY8uWLbNfddW/f3+eeuopFi5cyKOPPsqtt97K8ccfT0pKSvaxX3vtNTp37kyFChUOqmfAgAHMnz+fZcuW8eOPP/Laa6/Ro0cPUlNTs3uqKleunL39hg0bGDRoEO+88w7p6enMnz+fl19+GQgSw3bt2rF48WLOO+88nn32WQBmzJjBvffee1jXcu/evTz//POkpaXlio0ZM4YuXbpE3W/06NFcfPHFJCcn8/zzz3PXXXcBsGjRItatW8cll1xyWO0pTHrkRyk0PO3dmLGZL8aOiYhIwO63mLF/XvpP+rcKhthGLRzFTa/dFHNbv69grwesUqUKCxcu5IMPPmDOnDn07NmThx56iH79+h203dy5c+nQoQM1a9YEgiHQWC+Kz9WW8FWFu3bt4uOPP+aqq67Kjv3000/Z9b3wwgt07NiRyZMnc+utt+aqZ86cOTz88MPs3r2bbdu20aRJEy677LKYx50/f/5Bbe7Tpw/vv/8+l19+OUcffXT2vWmtWrVi9uzZAHTt2pWuXbsW6LxyuvXWWznvvPM499xzc7V7zJgxfPjhh1H3e/zxx3n99ddp27YtjzzyCHfeeSejRo3izjvvZNy4cYfVlsKmpK0Uql89JWbsu6axYyIiUnzKlStHhw4d6NChA82aNWP8+PG5krYj8dlnn9GoUSN+/vlnkpKSSE9Pz7VN165dueeee9i2bRsLFy7kggsuOCi+Z88ebr31VhYsWEDdunUZOnQoe/bsOew2VahQAbMgQS5Xrhz79u077LoA7r//fjZv3sw///nPg8qXLFnCjTfeyMyZMznhhBNy7bd582YWL16cfc9az549SUtLY+fOnSxbtowOHToA8M0339C1a1dmzJhBamq+rwotdEraREREcihoD1n/Vv2ze92OxBdffMFRRx1FgwYNAEhPT+eUU04BoGrVquzcuZMaNWrQtm1bBg4cyNatWznuuOOYOnUqLVq0yLf+9957j1GjRjFnzhyOO+446tevz9SpU7nqqqtwd5YsWUKLFi2oUqUKrVu3ZuDAgVx66aWUK1fuoHqyErQaNWqwa9cupk2bRo8ePQ5qZ05t2rThd7/7HVu2bKFatWpMmjSJ22+//YiuVzSjR4/mzTff5O233+aoow7c/fXf//6X7t278/zzz9OwYcOo+1arVo0dO3bw5Zdf0rBhQ2bPnk2jRo04/vjj2bJlS/Z2HTp04NFHHy2WhA2UtJVKo+cOBKK/OL7NkCCmF8eLiJQcu3bt4vbbb2f79u2UL1+e008/PXvSQP/+/UlLS8u+t23o0KG0b9+epKQkUlJSYtb5wgsv8OGHH7J7927q16/P9OnTadSoERA8BuOWW25h+PDh7N27l169emUnfz179uSqq67i3XffzVVnUlISv/3tb2natCm/+MUvaN26dXasX79+3HzzzVSuXJlPPvkku7xWrVo89NBDdOzYEXfnkksuoVu3bnlejxkzZrBgwQKGDRuWK5Y1cWLXrl0kJyczZswYOnfuzM0338wpp5xC+/btAejevTv33nsvw4YNY+vWrdlDveXLl8+eIHHxxRczevRoateuzbPPPsuVV17JUUcdRbVq1Rg7dmyebSwOljXGXZqkpqZ6UTxHxcyYEOfXQfVpmHTIx+gzMQmI/hqr3g2DWM7XWPVpmERp/C6IiBTE559/np3QiMRTtO+amS1093y77zR7VERERCQBKGkTERERSQBK2kREREQSgJI2ERER0H29EndH+h1T0iYiImVepUqV2Lp1qxI3iRt3Z+vWrUf0Kiw98qMUqlct9jN7tjXJ/3k+IiJlTXJyMpmZmWzevLm4myKlWKVKlUhOTj7s/ZW0lUIjurwXM/bGS7FjIiJlVYUKFahfv35xN0MkT3EbHjWzSmY2z8wWm9lyM7s/LB9nZmvMLD38pITlZmYjzSzDzJaYWcuIuvqa2arw0zdebRYREREpqeLZ0/YTcIG77zKzCsCHZjYzjP3R3afl2L4L0CD8tAX+AbQ1s+rAfUAq4MBCM5vh7t/Fse0iIiIiJUrceto8sCtcrRB+8rrDsxvwXLjfp0CSmdUCOgOz3X1bmKjNBtLi1e7SoM/EpOy3IuTUu2FS9lsRREREJHHEdfaomZUzs3RgE0HiNTcMjQiHQB83s4phWR1gXcTumWFZrHIRERGRMiOuSZu773f3FCAZaGNmTYG7gTOB1kB1YFBhHMvM+pvZAjNboNk/IiIiUtoUyXPa3H07MAdIc/eN4RDoT8C/gDbhZuuBuhG7JYdlscpzHmOUu6e6e2rNmjXjcBYiIiIixSees0drmllSuFwZ+BWwMrxPDTMz4HJgWbjLDOC6cBZpO2CHu28E3gQ6mVk1M6sGdArLRERERMqMePa01QLmmNkSYD7BPW2vARPMbCmwFKgBDA+3fx34CsgAngVuBXD3bcADYR3zgWFhmZRU/frBtJyTgwvBgw8eWF67Fpo2PbT9t26Fjh2hShUYMCD6Nl27Rq/3r38FM9iyJfp+aWmQlASXXnpw+d/+Bqefnve+IiIiBRC3R364+xLgrCjlF8TY3oHbYsTGAmMLtYGSeB58EO655/D3r1QJHngAli0LPjm9+GKQ0OW0bh3MmgUnnxy77j/+EXbvhn/+8+Dys88OErkOHQ6/3SIiIujdo6XSDW2e4IY2T0SNzR32BHOHPcG5t/Qm7Yrzufjidpw2eVwQfOaZIPnIMm7cgR6pBx6AM86Ac86Ba66BRx8tWGMWLoTzz4dWraBzZ9i4MSjv0AEGDYI2baBhQ/jgg6B89264+mpo3BiuuALatoUFC+Cuu+DHHyElBfr0Cbbdvx9++1to0gQ6dQrieTn22KD90d77tmsXPPYYDBmSO/b738PDDwe9ZbFceCFUrZq7/KyzoF69vNslIiJSAHqNVSl0wen9YsZW9wpi69K68b+kapTb8yOdr7yA6gBXXgnt28MjjwQbv/ACDB4M8+fD9OmweDHs3QstWwZJWH727oXbb4dXXoGaNQ/UNzbsNN23D+bNg9dfh/vvh7fegqefhmrVYMWKoDcsJSXY9qGHgqHG9PRgfe1aWLUKJk2CZ58NEr3p0+HXvw6ST4Cbby74Rfvzn+EPf4Bjjjm4/JVXoE4daKF3toqISPFS0lZGnfHcMyTPfg2AYzaupwEEidWpp8Knn0KDBrByZTC89+ST0K1b0ENVqRJcdlnBDvLFF0Hi9atfBev790OtWgfi3bsHP1u1CpIwgA8/hIEDg+WmTaF589j1169/IKmLrONQkjUIEsHVq+Hxxw/UAUGv34MPBkOjIiIixUxJWyn0TsY4IHqP22mTx1F1zSpqLF7IrCmz2V/5GC789SVUmvdRsEGvXjBlCpx5ZjA8mdeQYH7cg6HLTz6JHq8YPle5XLmg1+1QZe2fVUd+w6OxfPJJMARbr17Qjk2bguHbp56CNWsO9LJlZga9jPPmwS9+cXjHEhEROUy6p60UGjPvDsbMuyNqrO29d9D4X39n7/HHs7/yMRy3+ktqpC84sMEVVwRDgpMmBQkcBL1tr74Ke/YE93699lrBGnLGGbB584Gkbe9eWL48733OPjtIGiEYIl269ECsQoWgjsJ2yy2wYUPQy/bhh8E9du++C82aBQnc2rXBJzkZFi1SwiYiIsVCSVsZZfv2c0laG1o8OpQtKakHAtWqQaNG8PXXwSQBgNatg0dhNG8OXboEyczxx+d/kKOPDh79MWhQ0FuVkgIff5z3PrfeGiR6jRsHkwKaNDlwrP79gzZkTUSI5ZlnDtzXllO9enDnncEki+TkIDE8HAsWwI03Hlg/91y46ip4++2g3jfDRwmOHBmsZ2YGbY/cR0RE5BBY8KSN0iU1NdUXLFiQ/4ZHyMyY8OX2uB6jT8OkQz5G1sviJ/TOvV/Wy+In5qizT8Mk8vwu7NoVPA5j92447zwYNSoYKixs+/cHvWmVKgX3mV10UXBv3NFHF/6xRERESgAzW+juqfltp3vapGD69w96pfbsgb5945OwQZAUduwYJG7uwWxSJWwiIiJK2qSAJk7MXXbbbfDRRweXDRwIv/nN4R+natVg6FFEREQOoqRNDt/f/17cLRARESkzNBFBREREJAGop60UijYBIUvOCQgiIiKSGNTTJiIiIpIAlLSJiIiIJAAlbaXQ4JnnM3jm+VFjaVecT9oV0WMiIiJScumetlJo7XeLY8aqL48dExERkZJLPW0iIiIiCUBJm4iIiEgCUNImIiIikgCUtImIiIgkACVtIiIiIglAs0dLoY6n9Y0Zy7g6dkxERERKLiVtpdCNbZ+MGZs3PHZMRERESq64DY+aWSUzm2dmi81suZndH5bXN7O5ZpZhZi+Y2dFhecVwPSOM14uo6+6w/Asz6xyvNouIiIiUVPG8p+0n4AJ3bwGkAGlm1g74C/C4u58OfAfcEG5/A/BdWP54uB1m1hjoBTQB0oCnzaxcHNud8NZsS2fNtvSosWrL0qm2LHpMRERESq64JW0e2BWuVgg/DlwATAvLxwOXh8vdwnXC+IVmZmH5ZHf/yd3XABlAm3i1uzQY8kYHhrzRIWqsS/cOdOkePSYiIiIlV1xnj5pZOTNLBzYBs4HVwHZ33xdukgnUCZfrAOsAwvgO4ITI8ij7yCF6goHspnLU2LhxMGBAweuqUuXw2rBhA/Tokfc2a9fCxIkH1hcsgN/97vCOl5epU6FJEzjqqOAYWWbPhlatoFmz4Oc77wTlu3fDJZfAmWcG+911V/R69+6Fvn2D/Rs1gv/7v4Pj+/fDWWfBpZcW/jmJiEjpFNekzd33u3sKkEzQO3ZmvI5lZv3NbIGZLdi8eXO8DpPwnuAOdnNMsbahdm2YNi3vbXImbampMHJk4belaVN48UU477yDy2vUgFdfhaVLYfx4uPbaA7H/9/9g5Ur47DP46COYOTN3vVOnwk8/BfsvXAj//GdwTlmefDJI5kRERAqqSJ7T5u7bgTlAeyDJzLJmrSYD68Pl9UBdgDB+PLA1sjzKPpHHGOXuqe6eWrNmzXicRmL53zE88ttjuPuyKgy6pAqf/KcCbzx3NBuoTUfmMPzaYwF4b3oF/tCpCjCXjz7Ku8o1a6B9+6D3aMiQg2OPPAKtW0Pz5nDffUHZXXfB3/9+YJuhQ+HRR4PkpWnToGztWjj3XGjZMvh8/PGBfT/4AFJS4PHH4d13D/RKbdsGl18eHKtdO1iy5ED9118PHTrAqacWLMlr1AjOOCN3+VlnBcklBD1qP/4YJGHHHAMdOwblRx8dtDkzM/f+ZvDDD7BvX7Dv0UfDcccFscxM+M9/4MYb82+fiIhIlnjOHq1pZknhcmXgV8DnBMlb1uBYX+CVcHlGuE4Yf8fdPSzvFc4urQ80AObFq92lRkYaSSc6//fqLv7yn120OG8vadf9j9psYA4dGfL8D3y3yZg+shL3Tf4BOIcVK/KucuBAuOWWoPeoVq0D5bNmwapVMG8epKcHPUvvvw89e8KUKQe2mzIlKIt04onBUOSiRfDCCweGQB96KEjm0tPh978/eJ/77guSqiVL4MEH4brrDsRWroQ33wzacv/9wTAlwMUXB8Oyh2P69CA5q1jx4PLt24PeuAsvzL1Pjx5w7LHBdTr55KB3rnr1IHbHHfDww8GQrIiISEHF8zlttYDx4UzPo4Ap7v6ama0AJpvZcOAzYEy4/RjgeTPLALYRzBjF3Zeb2RRgBbAPuM3d98ex3aXDiUtZ9lJ5Jj1SibM67OXM1rkv2erF5WjUZh/HVXdgLz17wpdfxq7yo4+CBAaC4cJBg4LlWbOCz1lnBeu7dgVJ3A03wKZNQbK0eTNUqwZ16x48TLh3b3AfXXo6lCuX9/GzfPjhgXZccAFs3Qrffx+sX3JJkFxVrBgkhN9+C8nJ8Prr+dcbzfLlwXnOmnVw+b59cM01QZJ56qm595s3LzifDRvgu++CBPSii2DFiqBdrVoFvYciIiIFFbekzd2XAGdFKf+KKLM/3X0PcFWMukYAIwq7jaVajVWMeGkX6e+VZ+oTlWjSfh/dB/x0xNWa5S5zh7vvhptuyh276qrg/rVvvsndywbB0OdJJ8HixfDzz1Cp0pG1L7I3rFy5ILk6XJmZcMUV8NxzcNppB8f694cGDYJes2gmToS0NKhQIUjSzj47mOjw2WcwY0aQRO7ZEySbv/41/Pvfh99OEREpGzRAUwoNT3uXP7X6mKMrO+d028slN/7E2uXho+1OrsGr//gPAKe12M/n88uz8zsDyjN1at71nn02TJ4cLE+YcKC8c2cYOzboYQNYvz7oYYMgUZs8OUjcroqSku/YEQwhHnUUPP98MKsSoGpV2LkzejvOPffA8d99N5g0kHW/WGHZvj3otXvooeC8Iw0ZErT7iSdi73/yyQdmnP7wA3z6aTDj9P/+L0gG164NrssFFyhhExGRglHSVgrVr56CbWrOvT2qcHfXKrz0VEUuvzXoZTv/unIMfjiV4dceS7UTnStv38PQnscCH+U7m/HJJ4OJBc2aBYlZlk6doHfvA5MUevQ4kHA1aRIs16lz8H1wWW69NZid2aJFcD/ascH8CJo3D3rKWrQIeuMiDR0a3DfXvHkwYWH8+FzV5hLrnraXXgqGTz/5JEjSOofv2/jb3yAjA4YNCyZDpKQEiWhmJowYEQxztmwZlI8eHewzYwbce2+wfNttQRLbpEkwQeM3vwnaKyIicrgsuNe/dElNTfUFkQ/dihMzY8KX2+N6jD4Nk+J+jKzjlMbvgoiISElnZgvdPTW/7dTTVgqNnjuQ0XMHRo21GTKQNkOix0RERKTkUtJWCs1ZPZ45q6OPGZ4+ZTynT8l7PHHEiANDglmfEZoGIiIiUqzi+cgPSVCDBwcfERERKTnU0yYiIiKSAJS0iYiIiCQAJW0iIiIiCUBJm4iIiEgC0ESEUqhetRYxY9uaxI6JiIhIyaWkrRQa0eW9mLE3XoodExERkZJLw6MiIiIiCUBJm4iIiEgCUNJWCvWZmESfiUlRY70bJtG7YfSYiIiIlFxK2kREREQSgJI2ERERkQSgpE1EREQkAShpExEREUkAStpEREREEoCSNhEREZEEoDcilEI3tHkiZmzusNgxERERKbmUtJVCF5zeL2Zsda/YMRERESm5NDwqIiIikgDilrSZWV0zm2NmK8xsuZkNDMuHmtl6M0sPPxdH7HO3mWWY2Rdm1jmiPC0syzCzu+LV5tLinYxxvJMxLmrstMnjOG1y9JiIiIiUXPEcHt0H/MHdF5lZVWChmc0OY4+7+6ORG5tZY6AX0ASoDbxlZg3D8N+BXwGZwHwzm+HuK+LY9oQ2Zt4dQPRh0rb3BjENk4qIiCSWuCVt7r4R2Bgu7zSzz4E6eezSDZjs7j8Ba8wsA2gTxjLc/SsAM5scbqukTURERMqMIrmnzczqAWcBc8OiAWa2xMzGmlm1sKwOsC5it8ywLFa5iIiISJkR96TNzKoA04E73P174B/AaUAKQU/cXwvpOP3NbIGZLdi8eXNhVCkiIiJSYsQ1aTOzCgQJ2wR3fxHA3b919/3u/jPwLAeGQNcDdSN2Tw7LYpUfxN1HuXuqu6fWrFmz8E9GREREpBjFc/aoAWOAz939sYjyWhGbXQEsC5dnAL3MrKKZ1QcaAPOA+UADM6tvZkcTTFaYEa92i4iIiJREBZqIYGbN3H3pIdZ9NnAtsNTM0sOye4BrzCwFcGAtcBOAuy83sykEEwz2Abe5+/7w+AOAN4FywFh3X36IbRERERFJaAWdPfq0mVUExhEMde7Ibwd3/xCwKKHX89hnBDAiSvnree0nB5vQe3vM2MQvY8dERESk5CrQ8Ki7nwv0Ibi3bKGZTTSzX8W1ZSIiIiKSrcD3tLn7KmAIMAg4HxhpZivNrHu8GiciIiIigQIlbWbW3MweBz4HLgAuc/dG4fLjcWyfHIbBM89n8Mzzo8bSrjiftCtyx8qVr4CZxfVT95R6cT5zERGR0qug97Q9BYwG7nH3H7MK3X2DmQ2JS8vksK39bnHMWPXl0WP79+1lQpzvd+vTMCmu9YuIiJRmBU3aLgF+jJjNeRRQyd13u/vzcWudiIiIiAAFv6ftLaByxPoxYZmIiIiIFIGCJm2V3H1X1kq4fEx8miQiIiIiORU0afvBzFpmrZhZK+DHPLYXERERkUJU0Hva7gCmmtkGggfm/gLoGa9GiYiIiMjBCpS0uft8MzsTOCMs+sLd98avWXIkOp7WN2Ys4+rYMRERESm5CtrTBtAaqBfu09LMcPfn4tIqOSI3tn0yZmze8NgxERERKbkK+sL454HTgHRgf1jsgJI2ERERkSJQ0J62VKCxu3s8GyOFY822dADqV0/JFau2LIh91zR3TEREREqugiZtywgmH2yMY1ukkAx5owMAE3pvzxXr0j2ITYzz2w9ERESkcBU0aasBrDCzecBPWYXu3jUurRIRERGRgxQ0aRsaz0aIiIiISN4K+siP98zsFKCBu79lZscA5eLbNBERERHJUqA3IpjZb4FpwD/DojrAy3Fqk4iIiIjkUNDXWN0GnA18D+Duq4AT49UoERERETlYQZO2n9z9f1krZlae4DltIiIiIlIECjoR4T0zuweobGa/Am4FXo1fs+RIDE97N2Zs5ouxYyIiIlJyFTRpuwu4AVgK3AS8DoyOV6PkyER7qG4WPVRXREQkMRV09ujPwLPhR0RERESKWEHfPbqGKPewufuphd4iOWKj5w4Eor84vs2QIKYXx4uIiCSWgk5ESAVah59zgZHAv/PawczqmtkcM1thZsvNbGBYXt3MZpvZqvBntbDczGykmWWY2RIzaxlRV99w+1Vm1vdwTrQsmbN6PHNWj48aO33KeE6fEj0mIiIiJVeBkjZ33xrxWe/uTwCX5LPbPuAP7t4YaAfcZmaNCe6Pe9vdGwBvh+sAXYAG4ac/8A8IkjzgPqAt0Aa4LyvRExERESkrCjo82jJi9SiCnrc893X3jYQvmHf3nWb2OcFDebsBHcLNxgPvAoPC8ufc3YFPzSzJzGqF2852921hW2YDacCkgrRdREREpDQo6OzRv0Ys7wPWAlcX9CBmVg84C5gLnBQmdADfACeFy3WAdRG7ZYZlscpzHqM/QQ8dJ598ckGbJiIiIpIQCjp7tOPhHsDMqgDTgTvc/Xszi6zXzaxQHtLr7qOAUQCpqal68K+IiIiUKgUdHr0zr7i7PxZjvwoECdsEd38xLP7WzGq5+8Zw+HNTWL4eqBuxe3JYtp4Dw6lZ5e8WpN0iIiIipcWhzB69hQPDlTcDLYGq4ScXC7rUxgCf50jqZgBZM0D7Aq9ElF8XziJtB+wIh1HfBDqZWbVwAkKnsExiqFetBfWqtYga29akBduaRI+JiIhIyVXQe9qSgZbuvhPAzIYC/3H3X+exz9nAtcBSM0sPy+4BHgKmmNkNwNccuDfudeBiIAPYDfwGwN23mdkDwPxwu2FZkxIkuhFd3osZe+Ol2DEREREpuQqatJ0E/C9i/X8cmEAQlbt/CFiM8IVRtnfgthh1jQXGFqilIiIiIqVQQZO254B5ZvZSuH45weM6RERERKQIFHT26Agzm0nwNgSA37j7Z/FrlhyJPhOTAJjQe3uuWO+GQWzil7ljIiIiUnIVdCICwDHA9+7+JJBpZvXj1CYRERERyaFASZuZ3Ufw1oK7w6IK5PPuUREREREpPAXtabsC6Ar8AODuG4jxqA8RERERKXwFTdr+F87udAAzOzZ+TRIRERGRnAqatE0xs38CSWb2W+At4Nn4NUtEREREIuU7ezR8s8ELwJnA98AZwL3uPjvObRMRERGRUL5JW/hS99fdvRmgRC0B3NDmiZixucNix0RERKTkKujDdReZWWt3n5//plLcLji9X8zY6l6xYyIiIlJyFTRpawv82szWEswgNYJOuObxapiIiIiIHJBn0mZmJ7v7f4HORdQeKQTvZIwDove4nTY5iKnHTUREJLHk19P2MtDS3b82s+nufmURtEmO0Jh5dwDRk7a29wYxJW0iIiKJJb9HfljE8qnxbIiIiIiIxJZf0uYxlkVERESkCOU3PNrCzL4n6HGrHC7DgYkIx8W1dSIiIiIC5JO0uXu5omqIiIiIiMRW0NdYiYiIiEgxUtImIiIikgAK+nBdSSATem+PGZv4ZeyYiIiIlFzqaRMRERFJAEraRERERBKAkrZSaPDM8xk88/yosbQrziftiugxERERKbnilrSZ2Vgz22RmyyLKhprZejNLDz8XR8TuNrMMM/vCzDpHlKeFZRlmdle82luarP1uMWu/Wxw1Vn35Yqovjx4TERGRkiuePW3jgLQo5Y+7e0r4eR3AzBoDvYAm4T5Pm1k5MysH/B3oAjQGrgm3FRERESlT4jZ71N3fN7N6Bdy8GzDZ3X8C1phZBtAmjGW4+1cAZjY53HZFYbdXREREpCQrjnvaBpjZknD4tFpYVgdYF7FNZlgWq1xERESkTCnqpO0fwGlACrAR+GthVWxm/c1sgZkt2Lx5c2FVKyIiIlIiFGnS5u7fuvt+d/8ZeJYDQ6DrgboRmyaHZbHKo9U9yt1T3T21Zs2ahd94ERERkWJUpG9EMLNa7r4xXL0CyJpZOgOYaGaPAbWBBsA8wIAGZlafIFnrBfQuyjYnoo6n9Y0Zy7g6dkxERERKrrglbWY2CegA1DCzTOA+oIOZpQAOrAVuAnD35WY2hWCCwT7gNnffH9YzAHgTKAeMdffl8WpzaXFj2ydjxuYNjx0TERGRkiues0eviVI8Jo/tRwAjopS/DrxeiE0TERERSTh6I0IptGZbOmu2pUeNVVuWTrVl0WMiIiJSchXpPW1SNIa80QGACb2354p16R7EJn6ZOyYiIiIll3raRERERBKAkjYRERGRBKCkTURERCQBKGkTERERSQBK2kREREQSgJI2ERERkQSgR36UQsPT3o0Zm/li7JiIiIiUXEraSqH61VNixr5rGjsmIiIiJZeGR0VEREQSgJK2Umj03IGMnjswaqzNkIG0GRI9JiIiIiWXkrZSaM7q8cxZPT5q7PQp4zl9SvSYiIiIlFxK2kREREQSgJI2ERERkQSgpE1EREQkAShpExEREUkAStpEREREEoAerlsK1avWImZsW5PYMRERESm5lLSVQiO6vBcz9sZLsWMiIiJScml4VERERCQBKGkTERERSQBK2kqhPhOT6DMxKWqsd8MkejeMHhMREZGSS0mbiIiISAKIW9JmZmPNbJOZLYsoq25ms81sVfizWlhuZjbSzDLMbImZtYzYp2+4/Soz6xuv9oqIiIiUZPHsaRsHpOUouwt4290bAG+H6wBdgAbhpz/wDwiSPOA+oC3QBrgvK9ETERERKUvilrS5+/vAthzF3YDx4fJ44PKI8uc88CmQZGa1gM7AbHff5u7fAbPJnQiKiIiIlHpFfU/bSe6+MVz+BjgpXK4DrIvYLjMsi1UuIiIiUqYU20QEd3fAC6s+M+tvZgvMbMHmzZsLq1oRERGREqGo34jwrZnVcveN4fDnprB8PVA3YrvksGw90CFH+bvRKnb3UcAogNTU1EJLBhPRDW2eiBmbOyx2TEREREquok7aZgB9gYfCn69ElA8ws8kEkw52hIndm8CDEZMPOgF3F3GbE84Fp/eLGVvdK3ZMRERESq64JW1mNomgl6yGmWUSzAJ9CJhiZjcAXwNXh5u/DlwMZAC7gd8AuPs2M3sAmB9uN8zdc05uEBERESn14pa0ufs1MUIXRtnWgdti1DMWGFuITSv13skYB0TvcTttchBTj5uIiEhiKerhUSkCY+bdAURP2treG8SUtImIiCQWvcZKREREJAEoaRMRERFJAEraRERERBKAkjYRERGRBKCkTURERCQBKGmTIlOufAXMLK6fuqfUK+7TFBERiQs98qMUmtB7e8zYxC9jx+Jt/769TIjz8fs0TIpr/SIiIsVFPW0iIiIiCUBJm4iIiEgCUNJWCg2eeT6DZ54fNZZ2xfmkXRE9JiIiIiWX7mkrhdZ+tzhmrPry2DEREREpudTTJiIiIpIAlLSJiIiIJAAlbSIiIiIJQEmbiIiISAJQ0iYiIiKSADR7tBTqeFrfmLGMq2PHREREpORS0lYK3dj2yZixecNjx0RERKTk0vCoiIiISAJQ0lYKrdmWzppt6VFj1ZalU21Z9JiIiIiUXBoeLYWGvNEBgAm9t+eKdekexCZ+mTsmIiIiJZd62kREREQSQLEkbWa21syWmlm6mS0Iy6qb2WwzWxX+rBaWm5mNNLMMM1tiZi2Lo80iIiIixak4e9o6unuKu6eG63cBb7t7A+DtcB2gC9Ag/PQH/lHkLRUREREpZiVpeLQbMD5cHg9cHlH+nAc+BZLMrFYxtE9ERESk2BRX0ubALDNbaGb9w7KT3H1juPwNcFK4XAdYF7FvZlgmIiIiUmYU1+zRc9x9vZmdCMw2s5WRQXd3M/NDqTBM/voDnHzyyYXXUhEREZESoFiSNndfH/7cZGYvAW2Ab82slrtvDIc/N4WbrwfqRuyeHJblrHMUMAogNTX1kBK+0mZ42rsxYzNfjB0TERGRkqvIh0fN7Fgzq5q1DHQClgEzgKwXY/YFXgmXZwDXhbNI2wE7IoZRJYr61VOoXz0lauy7pil81zR6TEREREqu4uhpOwl4ycyyjj/R3d8ws/nAFDO7AfgauDrc/nXgYiAD2A38puibLCIiIlK8ijxpc/evgBZRyrcCF0Ypd+C2ImhaqTF67kAg+ovj2wwJYnpxvIiISGIpSY/8kEIyZ/V45qweHzV2+pTxnD4lekxERERKLiVtIiIiIglASZuIiIhIAlDSJiIiIpIAlLSJiIiIJAAlbSIiIiIJoLheYyVxVK9arieqZNvWJHZMRERESi4lbaXQiC7vxYy98VLsmIiIiJRcGh4VERERSQBK2qRUKVe+AmYW90/dU+oV96mKiEgZo+HRUqjPxCQAJvTenivWu2EQm/hl7lhpsH/fXiYUwbn1Ca+jiIhIUVFPm4iIiEgCUNImIiIikgCUtImIiIgkACVtIiIiIglASZvIYSiKWaqaoSoiIpE0e1TkMBTFLFXNUBURkUhK2kqhG9o8ETM2d1jsmIiIiJRcStpKoQtO7xcztrpX7JiIiIiUXLqnTaSE0n1zIiISST1tpdA7GeOA6D1up00OYupxK/l035yIiERS0lYKjZl3BxA9aWt7bxBT0iZwoDcv3pJPPoV1X6+N+3FEREozJW0iZZje1Soikjh0T5uIxJ3uzxMROXIJ09NmZmnAk0A5YLS7P1TMTRKRAtL9eSIiRy4hetrMrBzwd6AL0Bi4xswaF2+rRERERIpOQiRtQBsgw92/cvf/AZOBbsXcJhEpQTQEKyKlXaIMj9YB1kWsZwJti6ktIlICFcUQ7HWNaxbJbNsKFSux96c9cT2GZvSKJB5z9+JuQ77MrAeQ5u43huvXAm3dfUDENv2B/uHqGcAXcW5WDWBLnI9R0uka6Bpk0XXQNcii66BrkEXXoeDX4BR3r5nfRonS07YeqBuxnhyWZXP3UcCoomqQmS1w99SiOl5JpGuga5BF10HXIIuug65BFl2Hwr8GiXJP23yggZnVN7OjgV7AjGJuk4iIiEiRSYieNnffZ2YDgDcJHvkx1t2XF3OzRERERIpMQiRtAO7+OvB6cbcjQpENxZZguga6Bll0HXQNsug66Bpk0XUo5GuQEBMRRERERMq6RLmnTURERKRMU9J2iMwszcy+MLMMM7uruNsTL2ZW18zmmNkKM1tuZgPD8upmNtvMVoU/q4XlZmYjw+uyxMxaFu8ZFC4zK2dmn5nZa+F6fTObG57vC+EEGcysYrieEcbrFWvDC4mZJZnZNDNbaWafm1n7svhdMLPfh/89LDOzSWZWqSx8F8xsrJltMrNlEWWH/Ps3s77h9qvMrG9xnMvhinENHgn/m1hiZi+ZWVJE7O7wGnxhZp0jyhP6b0i06xAR+4OZuZnVCNfLzHchLL89/D4sN7OHI8oL77vg7voU8EMwCWI1cCpwNLAYaFzc7YrTudYCWobLVYEvCV4h9jBwV1h+F/CXcPliYCZgQDtgbnGfQyFfjzuBicBr4foUoFe4/AxwS7h8K/BMuNwLeKG4215I5z8euDFcPhpIKmvfBYKHfK8BKkd8B/qVhe8CcB7QElgWUXZIv3+gOvBV+LNauFytuM/tCK9BJ6B8uPyXiGvQOPz7UBGoH/7dKFca/oZEuw5heV2CyYJfAzXK4HehI/AWUDFcPzEe3wX1tB2aMvM6LXff6O6LwuWdwOcEf7S6EfwBJ/x5ebjcDXjOA58CSWZWq2hbHR9mlgxcAowO1w24AJgWbpLzOmRdn2nAheH2CcvMjif4R2oMgLv/z923Uwa/CwSTtyqbWXngGGAjZeC74O7vA9tyFB/q778zMNvdt7n7d8BsIC3ujS8k0a6Bu89y933h6qcEzxCF4BpMdvef3H0NkEHw9yPh/4bE+C4APA78CYi8Ub7MfBeAW4CH3P2ncJtNYXmhfheUtB2aaK/TqlNMbSky4bDOWcBc4CR33xiGvgFOCpdL87V5guAfo5/D9ROA7RH/WEeea/Z1COM7wu0TWX1gM/AvC4aIR5vZsZSx74K7rwceBf5LkKztABZStr4LkQ71918qvxcRrifoVYIydg3MrBuw3t0X5wiVpevQEDg3vBXiPTNrHZYX6jVQ0iZ5MrMqwHTgDnf/PjLmQd9vqZ5+bGaXApvcfWFxt6UYlScYCviHu58F/EAwHJatjHwXqhH8n3B9oDZwLAnUOxBPZeH3nxczGwzsAyYUd1uKmpkdA9wD3FvcbSlm5QmGe9sBfwSmxKNnXUnbocn3dVqliZlVIEjYJrj7i2Hxt1lDXeHPrC7g0nptzga6mtlagu7rC4AnCbr5s55zGHmu2dchjB8PbC3KBsdBJpDp7nPD9WkESVxZ+y5cBKxx983uvhd4keD7UZa+C5EO9fdfKr8XZtYPuBToEyavULauwWkE/yOzOPx3MhlYZGa/oGxdh0zgxXAoeB7ByEwNCvkaKGk7NGXmdVrh/yGMAT5398ciQjOArJk+fYFXIsqvC2cLtQN2RAydJCx3v9vdk929HsHv+x137wPMAXqEm+W8DlnXp0e4fUL3QLj7N8A6MzsjLLoQWEEZ+y4QDIu2M7Njwv8+sq5Dmfku5HCov/83gU5mVi3stewUliUsM0sjuHWiq7vvjgjNAHpZMIO4PtAAmEcp/Bvi7kvd/UR3rxf+O5lJMIntG8rQdwF4mWAyAmbWkGBywRYK+7tQmDMqysKHYDbMlwSzPgYXd3vieJ7nEAx3LAHSw8/FBPfkvA2sIpgpUz3c3oC/h9dlKZBa3OcQh2vSgQOzR08N/8PLAKZyYMZQpXA9I4yfWtztLqRzTwEWhN+HlwlmfJW57wJwP7ASWAY8TzAjrNR/F4BJBPfx7SX4o3zD4fz+Ce77ygg/vynu8yqEa5BBcF9S1r+Rz0RsPzi8Bl8AXSLKE/pvSLTrkCO+lgOzR8vSd+Fo4N/hvw2LgAvi8V3QGxFEREREEoCGR0VEREQSgJI2ERERkQSgpE1EREQkAShpExEREUkAStpEREREEoCSNhGJOzPbFef67wifzH7Exwufp/SWmaWbWc8csXbha2rSzexzMxt6BMe553D3FZGySY/8EJG4M7Nd7l4ljvWvJXgG1JYjPV74ENDh7n5RlNgXwNXuvtjMygFnuPuKwzxOXK+JiJQ+6mkTkWJhZqeZ2RtmttDMPjCzM8PycWY20sw+NrOvzKxHWH6UmT1tZivNbLaZvW5mPczsdwTvAp1jZnMi6h9hZovN7FMzOynK8aub2ctmtiTcprmZnUjwgMzWYW/aaTl2O5HgoZq4+/6shM3MjjWzsWY2z8w+s+AF2phZPzN7MTzPVWb2cFj+EFA5PMaEsOzX4f7pZvbPMCnEzHZFOxczO8nMXgrLF5vZL2PVE37GmdkyM1tqZr8vpF+jiBQhJW0iUlxGAbe7eyvg/wFPR8RqEbyV41LgobCsO1APaAxcC7QHcPeRwAago7t3DLc9FvjU3VsA7wO/jXL8+4HP3L05wQuvn3P3TcCNwAfunuLuq3Ps8zjwRZgs3WRmlcLywQSvqWpD8CqbR8zs2DCWAvQEmgE9zayuu98F/Bgeo4+ZNQq3OdvdU4D9QJ98zmUk8F5Y3hJYnkc9KUAdd2/q7s2Af0W5HiJSwpXPfxMRkcJlZlWAXwJTzSyruGLEJi+7+8/AiohesnOAqWH5N5G9alH8D3gtXF4I/CrKNucAVwK4+ztmdoKZHZdXu919WNgz1gnoDVxD8HqzTkBXM/t/4aaVgJPD5bfdfUd43iuAUwhefRTpQqAVMD+8HpU58AL2WOdyAXBd2K79wA4zuzZGPa8Cp5rZU8B/gFl5naeIlExK2kSkOBwFbA97g6L5KWLZYmyTl71+4Ibd/RTiv3Vh79s/zOxZYLOZnRC28Up3/yJyWzNry8HnEqstBox397ujxA7lXGLWY2YtgM7AzcDVBO9+FJEEouFRESly7v49sMbMrgKwQIt8dvsIuDK8t+0kgh6uLDuBqofYjA8IhyDNrAOwJWxXTGZ2iR3oGmxAkERtB94Ebs+KmdlZBTj+XjOrEC6/DfQI76nLut/ulHz2fxu4Jdy+nJkdH6seM6sBHOXu04EhBMOpIpJg1NMmIkXhGDPLjFh/jCBh+oeZDQEqAJOBxXnUMZ1gGHEFwfDiImBHGBsFvGFmGyLua8vPUGCsmS0BdgN9C7DPtcDjZrYb2Af0cff9ZvYA8ASwxMyOAtYQ3I+Xl1Hh9ovC+9qGALPC/fcCtwFf57H/QGCUmd1AkDze4u6fxKjnR+BfYRlAtB49ESnh9MgPEUkYZlbF3XeFQ5LzCG64/6a42yUiUhTU0yYiieQ1M0sCjgYeUMImImWJetpEREREEoAmIoiIiIgkACVtIiIiIglASZuIiIhIAlDSJiIiIpIAlLSJiIiIJAAlbSIiIiIJ4P8D/1BWKA/m5BIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 전처리\n",
    "corpus = [item['completion'] for item in list_data_dict_1]\n",
    "\n",
    "# 문장의 길이 평균 계산\n",
    "avg_length = sum(len(sentence) for sentence in corpus) / len(corpus)\n",
    "\n",
    "# 문장의 길이 표준 편차\n",
    "std_deviation = np.std([len(sentence) for sentence in corpus])\n",
    "\n",
    "# 길이 분포 분석\n",
    "lengths = [len(sentence) for sentence in corpus]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(lengths, bins=20, color='skyblue')\n",
    "plt.axvline(avg_length, color='red', linestyle='dashed', linewidth=2, label=f'Average Length: {avg_length:.2f}')\n",
    "plt.text(avg_length, plt.gca().get_ylim()[1]*0.8, f'avg_length: {avg_length:.2f}', color='red', ha='left')\n",
    "\n",
    "plt.axvline(std_deviation, color='green', linestyle='dashed', linewidth=2, label=f'Std Deviation: {std_deviation:.2f}')\n",
    "plt.text(std_deviation, plt.gca().get_ylim()[1]*0.7, f'std_deviation: {std_deviation:.2f}', color='blue', ha='left')\n",
    "\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8da1e418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete sentences ratio: 0.89\n",
      "Incomplete sentences ratio: 0.11\n"
     ]
    }
   ],
   "source": [
    "# 문장의 완성도 분석\n",
    "# 완성도는 주관적인 척도이므로 여기서는 문장의 마침표의 유무로 간단하게 판단\n",
    "complete_sentences = [sentence for sentence in corpus if sentence.endswith('.')]\n",
    "incomplete_sentences = [sentence for sentence in corpus if not sentence.endswith('.')]\n",
    "complete_ratio = len(complete_sentences) / len(corpus)\n",
    "incomplete_ratio = len(incomplete_sentences) / len(corpus)\n",
    "\n",
    "print(f'Complete sentences ratio: {complete_ratio:.2f}')\n",
    "print(f'Incomplete sentences ratio: {incomplete_ratio:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41d2debb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?',\n",
       "  'completion_0': 'Allow me to answer your question. I know that you are curious about me.',\n",
       "  'completion_1': '번디는 다양한 인터뷰자들과 뉴스홍보 담당자들과의 면담 때 밝혔다.',\n",
       "  'completion_2': '라이언에게 말했다.',\n",
       "  'ranking': [2, 1, 0]},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?',\n",
       "  'completion_0': '개포주공아파트는 다섯 단지로 이루어져 있습니다.',\n",
       "  'completion_1': '이날 목송에서 구글상위노',\n",
       "  'completion_2': '개포주공아파트는 총 27개 단지로 이루어져 있습니다.',\n",
       "  'ranking': [2, 0, 1]},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?',\n",
       "  'completion_0': 'The diameter of the Metallic domain is bigger than the Hyperonic domain.',\n",
       "  'completion_1': '이 질문은 조금 불분명합니다. 김영삼 대통령이 후보 시절에 어떤 발언을 했고, 누가 그 발언을 문제삼았는지에 따라 답이 다를 수 있습니다.\\\\n\\\\n만약 김영삼 대통령이 후보 시절에 지역표심을 겨냥한 발언을 했다는 가정하에, 그 발언을 문제삼은 후보가 누구였는지를 대답하자면, 그 답은 이화선 당시 민주당 대통령 후보가 될 것입니다. 1992년 총선 때, 김영삼 대선후보는 \"집값이 오른 노량진역 부근의 부동산 가격은 세월호 폭침 후 \\\\\\'강남 도시재생\\\\\\' 일환으로 상승했다\"는 발언을 했습니다. 하지만 이화선 후보는 이 발언을 \"전국적으로 경제적 발전이 이루어지지 않은 지방민의 마음을 멀리해지려는 무례한 발언\"이라고 비판하며 문제삼았습니다.\\\\n\\\\n하지만, 이 질문을 답변하는 데 있어서 보다 명확한 정보가 있으면 답변을 보완할 수 있습니다.',\n",
       "  'completion_2': '김영삼의 후보 시절에 지역표심을 겨냥한 발언은 대통령 당선 전까지 대한민국 정부가 추구하고 있는 민주주의 광범위하게 확립과 보수의 사상을 이어가는 데 있어 지역경제 발전과 공공서비스 신속 개선을 위해 합리적인 국가 정책에 따르는 방향성을 제시하고 있습니다.',\n",
       "  'ranking': [1, 2, 0]}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_2_RM = '/aiffel/aiffel/NLP/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl'\n",
    "with open(data_path_2_RM, 'r', encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict_2 = json.load(json_file)\n",
    "    \n",
    "print(len(list_data_dict_2))\n",
    "list_data_dict_2[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b771ec89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모든 완성 문장을 하나의 리스트로 펼치기\n",
    "corpus = []\n",
    "for item in list_data_dict_2:\n",
    "    for i in range(3):  # completion0, completion1, completion2\n",
    "        if f'completion_{i}' in item:\n",
    "            corpus.append(item[f'completion_{i}'])\n",
    "\n",
    "# 문장의 길이 평균 계산\n",
    "avg_length = sum(len(sentence) for sentence in corpus) / len(corpus)\n",
    "\n",
    "# 문장의 길이 표준 편차\n",
    "std_deviation = np.std([len(sentence) for sentence in corpus])\n",
    "\n",
    "# 길이 분포 분석\n",
    "lengths = [len(sentence) for sentence in corpus]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(lengths, bins=20, color='skyblue')\n",
    "plt.axvline(avg_length, color='red', linestyle='dashed', linewidth=2, label=f'Average Length: {avg_length:.2f}')\n",
    "plt.text(avg_length, plt.gca().get_ylim()[1]*0.8, f'avg_length: {avg_length:.2f}', color='red', ha='left')\n",
    "\n",
    "plt.axvline(std_deviation, color='green', linestyle='dashed', linewidth=2, label=f'Std Deviation: {std_deviation:.2f}')\n",
    "plt.text(std_deviation, plt.gca().get_ylim()[1]*0.7, f'std_deviation: {std_deviation:.2f}', color='blue', ha='left')\n",
    "\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8b78193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = '/aiffel/aiffel/NLP/KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, 'r', encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict_3 = json.load(json_file)\n",
    "    \n",
    "print(len(list_data_dict_3))\n",
    "list_data_dict_3[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a52a4314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 전처리\n",
    "corpus = [item['prompt'] for item in list_data_dict_3]\n",
    "\n",
    "# 문장의 길이 평균 계산\n",
    "avg_length = sum(len(sentence) for sentence in corpus) / len(corpus)\n",
    "\n",
    "# 문장의 길이 표준 편차\n",
    "std_deviation = np.std([len(sentence) for sentence in corpus])\n",
    "\n",
    "# 길이 분포 분석\n",
    "lengths = [len(sentence) for sentence in corpus]\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(lengths, bins=20, color='skyblue')\n",
    "plt.axvline(avg_length, color='red', linestyle='dashed', linewidth=2, label=f'Average Length: {avg_length:.2f}')\n",
    "plt.text(avg_length, plt.gca().get_ylim()[1]*0.8, f'avg_length: {avg_length:.2f}', color='red', ha='left')\n",
    "\n",
    "plt.axvline(std_deviation, color='green', linestyle='dashed', linewidth=2, label=f'Std Deviation: {std_deviation:.2f}')\n",
    "plt.text(std_deviation, plt.gca().get_ylim()[1]*0.7, f'std_deviation: {std_deviation:.2f}', color='blue', ha='left')\n",
    "\n",
    "plt.title('Distribution of Sentence Lengths')\n",
    "plt.xlabel('Length of Sentences')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263efab5",
   "metadata": {},
   "source": [
    "기존 모델에서 data_path_3_PPO 있는 문장을 넣어 출력되는 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d2bc57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불고기용 고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜요?\" \"고기 한우에요?\" \"네.\" \"근데 왜\n"
     ]
    }
   ],
   "source": [
    "# 디코딩 성능 확인\n",
    "\n",
    "max_length=144\n",
    "# prompt를 리스트에 담기\n",
    "prompts = [list_data_dict_1[0]['prompt']]\n",
    "\n",
    "# prompts를 사용하여 토크나이저에 전달\n",
    "input_ids = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)[\"input_ids\"].to(device)\n",
    "\n",
    "# 나머지 코드는 그대로 유지\n",
    "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
    "print(tokenizer.decode(output_greedy[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dc1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#q-beam, top_k, top_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73f16d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "259b6415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del output_greedy, input_ids, model , tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed4f25",
   "metadata": {},
   "source": [
    "### SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f33cc6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cd74c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pillow==5.3\n",
      "  Downloading Pillow-5.3.0.tar.gz (15.6 MB)\n",
      "     |████████████████████████████████| 15.6 MB 5.8 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: pillow\n",
      "  Building wheel for pillow (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pillow: filename=Pillow-5.3.0-cp39-cp39-linux_x86_64.whl size=495662 sha256=ab07f4736028eb4e5fe5e7d8e9a89318ea331f1c04ccab79d25bf6e9f6a232af\n",
      "  Stored in directory: /aiffel/.cache/pip/wheels/67/fc/36/d4f512254f64d51dd2718711f345cca21e04009e5ed0e159a8\n",
      "Successfully built pillow\n",
      "Installing collected packages: pillow\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 8.3.0\n",
      "    Uninstalling Pillow-8.3.0:\n",
      "      Successfully uninstalled Pillow-8.3.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "matplotlib 3.4.3 requires pillow>=6.2.0, but you have pillow 5.3.0 which is incompatible.\u001b[0m\n",
      "Successfully installed pillow-8.2.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install pillow==5.3 # 8.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231d85e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
      "     |████████████████████████████████| 199 kB 7.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from peft) (1.21.4)\n",
      "Collecting torch>=1.13.0\n",
      "  Downloading torch-2.2.2-cp39-cp39-manylinux1_x86_64.whl (755.5 MB)\n",
      "     |████████████████████████████████| 755.5 MB 8.8 kB/s             \n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from peft) (21.3)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.9/site-packages (from peft) (4.28.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from peft) (5.8.0)\n",
      "Collecting accelerate>=0.21.0\n",
      "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "     |████████████████████████████████| 290 kB 66.2 MB/s            \n",
      "\u001b[?25hCollecting safetensors\n",
      "  Downloading safetensors-0.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 62.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from peft) (6.0)\n",
      "Collecting huggingface-hub>=0.17.0\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl (388 kB)\n",
      "     |████████████████████████████████| 388 kB 63.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from peft) (4.62.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (2.26.0)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "     |████████████████████████████████| 171 kB 92.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.17.0->peft) (4.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->peft) (3.0.6)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "     |████████████████████████████████| 56.5 MB 83 kB/s              \n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "     |████████████████████████████████| 166.0 MB 40 kB/s              \n",
      "\u001b[?25hCollecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "     |████████████████████████████████| 167.9 MB 31 kB/s              \n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     |████████████████████████████████| 5.7 MB 83.7 MB/s            \n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "     |████████████████████████████████| 121.6 MB 16 kB/s              \n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "     |████████████████████████████████| 99 kB 145 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (3.0.3)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "     |████████████████████████████████| 14.1 MB 91.0 MB/s            \n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "     |████████████████████████████████| 823 kB 94.2 MB/s            \n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "     |████████████████████████████████| 196.0 MB 24 kB/s              \n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "     |████████████████████████████████| 124.2 MB 13 kB/s              \n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.9/site-packages (from torch>=1.13.0->peft) (2.6.3)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "     |████████████████████████████████| 410.6 MB 2.7 kB/s              \n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "     |████████████████████████████████| 731.7 MB 6.6 kB/s               0:20:33 ��███████                   | 296.1 MB 545 kB/s eta 0:13:19 \n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "     |████████████████████████████████| 23.7 MB 66.8 MB/s            \n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "     |████████████████████████████████| 21.1 MB 70.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (2021.11.10)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.9/site-packages (from transformers->peft) (0.13.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->torch>=1.13.0->peft) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.10)\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     |████████████████████████████████| 536 kB 69.6 MB/s            \n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-cusparse-cu12, nvidia-cublas-cu12, mpmath, typing-extensions, triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusolver-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, fsspec, torch, safetensors, huggingface-hub, accelerate, peft\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 4.7.0\n",
      "    Uninstalling typing-extensions-4.7.0:\n",
      "      Successfully uninstalled typing-extensions-4.7.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.11.1\n",
      "    Uninstalling fsspec-2021.11.1:\n",
      "      Successfully uninstalled fsspec-2021.11.1\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.12.1\n",
      "    Uninstalling torch-1.12.1:\n",
      "      Successfully uninstalled torch-1.12.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.15.1\n",
      "    Uninstalling huggingface-hub-0.15.1:\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.20.3\n",
      "    Uninstalling accelerate-0.20.3:\n",
      "      Successfully uninstalled accelerate-0.20.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.10.0 which is incompatible.\u001b[0m\n",
      "Successfully installed accelerate-0.28.0 fsspec-2024.3.1 huggingface-hub-0.22.1 mpmath-1.3.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 peft-0.10.0 safetensors-0.4.2 sympy-1.12 torch-2.2.2 triton-2.2.0 typing-extensions-4.10.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# !pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29c9be94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "CUDA SETUP: Loading binary /opt/conda/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_905/70331579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mquantization_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbnb_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    472\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2793\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2794\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2795\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2796\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, load_in_8bit, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlow_cpu_mem_usage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3123\u001b[0;31m                     new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   3124\u001b[0m                         \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3125\u001b[0m                         \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, load_in_8bit, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    696\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_in_8bit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;31m# For backward compatibility with older versions of `accelerate`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 698\u001b[0;31m             \u001b[0mset_module_tensor_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mset_module_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    699\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint8\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SCB\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/accelerate/utils/modeling.py\u001b[0m in \u001b[0;36mset_module_tensor_to_device\u001b[0;34m(module, tensor_name, device, value, dtype, fp16_statistics, tied_params_map)\u001b[0m\n\u001b[1;32m    385\u001b[0m                     \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mold_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mnew_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "# from transformers import BitsAndBytesConfig\n",
    "# from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "#                                          model_max_length=122)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\"     # Allow batched inference\n",
    "\n",
    "# # load model and finetune\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "# #     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map={\"\":0},\n",
    "# )\n",
    "\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model = prepare_model_for_kbit_training(model_name)\n",
    "\n",
    "# lora_config = LoraConfig(\n",
    "#     r=train_args.lora_r,\n",
    "#     lora_alpha=train_args.lora_alpha,\n",
    "#     target_modules=train_args.lora_target_modules,\n",
    "#     lora_dropout=train_args.lora_dropout,\n",
    "#     bias=\"none\",\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c31bed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2+cu121\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a048b5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/ko-gpt-trinity-1.2B-v0.5', vocab_size=51200, model_max_length=144, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=max_length, #512 -> 122 평균 길이로 변경\n",
    ")\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b6dbc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb6ca78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5005ad43",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b50a036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([30132, 42872, 33313, 30679, 40479, 39911,   384, 22509, 21921, 25372,\n",
      "          385, 31245, 23280, 34957, 25617, 36539, 29991, 25624, 25400, 31167,\n",
      "          376, 42872,   379, 46803,   456, 30303, 35353,   384, 25785, 20573,\n",
      "        37780,   383, 46900, 43226,   565, 27071, 23151, 31555, 41690, 35071,\n",
      "        25400, 31269, 32677, 30765, 31810, 36229, 30326, 33889, 30093, 34957,\n",
      "        25617, 30021, 30434, 29991, 39687, 34036, 19016, 31997, 49906, 19352,\n",
      "        30011, 30904, 36731, 43502, 30228, 31214, 30326, 29991, 31621, 33314,\n",
      "        34347, 30843, 50342, 33512, 31370, 34243, 29991, 35144, 32586, 32622,\n",
      "        44680, 30110, 21844, 39826, 34803, 31356, 39075, 30242, 36966, 29985,\n",
      "        34179, 36513, 30718, 35557, 32361, 31018, 29404, 35942, 19352, 41049,\n",
      "            1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,   383, 46900, 43226,   565, 27071, 23151, 31555, 41690, 35071,\n",
      "        25400, 31269, 32677, 30765, 31810, 36229, 30326, 33889, 30093, 34957,\n",
      "        25617, 30021, 30434, 29991, 39687, 34036, 19016, 31997, 49906, 19352,\n",
      "        30011, 30904, 36731, 43502, 30228, 31214, 30326, 29991, 31621, 33314,\n",
      "        34347, 30843, 50342, 33512, 31370, 34243, 29991, 35144, 32586, 32622,\n",
      "        44680, 30110, 21844, 39826, 34803, 31356, 39075, 30242, 36966, 29985,\n",
      "        34179, 36513, 30718, 35557, 32361, 31018, 29404, 35942, 19352, 41049,\n",
      "            1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='/aiffel/aiffel/NLP/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "399f7ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(train_dataset.input_ids[0].tolist(), skip_special_tokens=True)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b05ffd",
   "metadata": {},
   "source": [
    "기존 모델의 결과물은 \n",
    "반복적인 말만 출력했었는데\n",
    "\n",
    "SFT를 적용한 모델의 결과물은\n",
    "말이되는 문장으로 출력이 되어서 나왔다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b711d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">14</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">11 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>fp16 = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">True</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">12 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>)                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>14 trainer = lora.get_Trainer(                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model=model,                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>args=training_args,                                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>data_collator=data_collator,                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span>module <span style=\"color: #008000; text-decoration-color: #008000\">'loralib'</span> has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'get_Trainer'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m14\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m11 \u001b[0m\u001b[2m│   \u001b[0mfp16 = \u001b[94mTrue\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m12 \u001b[0m\u001b[2m│   \u001b[0m)                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m14 trainer = lora.get_Trainer(                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   \u001b[0mmodel=model,                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   \u001b[0margs=training_args,                                                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0mdata_collator=data_collator,                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0mmodule \u001b[32m'loralib'\u001b[0m has no attribute \u001b[32m'get_Trainer'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/aiffel/aiffel/NLP/KoChatGPT/test_2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91f0a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 trainer.train()                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>model.save_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">'/aiffel/aiffel/NLP/KoChatGPT/output_1_SFT_quest'</span>)                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1991</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1988 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span>xm.optimizer_step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1989 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.do_grad_scaling:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1990 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_before = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1991 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1992 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.update()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1993 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_after = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1994 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>optimizer_was_run = scale_before &lt;= scale_after                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">338</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">335 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>]) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"No inf checks were rec</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>338 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retval = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">339 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"stage\"</span>] = OptState.STEPPED                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">341 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">285</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, optimizer_state, *args, **kwargs):                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retval = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">284 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sum</span>(v.item() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>].values()):    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>285 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>retval = optimizer.step(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> retval                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, *args, **kwargs):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lr_scheduler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">65</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  62 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>instance = instance_ref()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>instance._step_count += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>wrapped = func.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__get__</span>(instance, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  65 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapped(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  66 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  67 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Note that the returned function here is no longer a bound method,</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  68 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># so attributes like `__func__` and `__self__` no longer exist.</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">113</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>obj, *_ = args                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>profile_name = <span style=\"color: #808000; text-decoration-color: #808000\">\"Optimizer.step#{}.step\"</span>.format(obj.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.profiler.record_function(profile_name):                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>113 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hooked = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.step, <span style=\"color: #808000; text-decoration-color: #808000\">\"hooked\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">436</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Exponential moving average of gradient values</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">434 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg\"</span>] = torch.zeros_like(p.data)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Exponential moving average of squared gradient values</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>436 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg_sq\"</span>] = torch.zeros_like(p.data)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">438 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>exp_avg, exp_avg_sq = state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg\"</span>], state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg_sq\"</span>]                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">439 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>beta1, beta2 = group[<span style=\"color: #808000; text-decoration-color: #808000\">\"betas\"</span>]                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.76</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.24</span> GiB already \n",
       "allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.75</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.93</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 trainer.train()                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mmodel.save_pretrained(\u001b[33m'\u001b[0m\u001b[33m/aiffel/aiffel/NLP/KoChatGPT/output_1_SFT_quest\u001b[0m\u001b[33m'\u001b[0m)                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1991\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1988 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mxm.optimizer_step(\u001b[96mself\u001b[0m.optimizer)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1989 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.do_grad_scaling:                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1990 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_before = \u001b[96mself\u001b[0m.scaler.get_scale()                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1991 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.step(\u001b[96mself\u001b[0m.optimizer)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1992 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.update()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1993 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_after = \u001b[96mself\u001b[0m.scaler.get_scale()                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1994 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer_was_run = scale_before <= scale_after                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m338\u001b[0m in \u001b[92mstep\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m335 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m]) > \u001b[94m0\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mNo inf checks were rec\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m337 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m338 \u001b[2m│   │   \u001b[0mretval = \u001b[96mself\u001b[0m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m339 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer_state[\u001b[33m\"\u001b[0m\u001b[33mstage\u001b[0m\u001b[33m\"\u001b[0m] = OptState.STEPPED                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m341 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m285\u001b[0m in \u001b[92m_maybe_opt_step\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_maybe_opt_step\u001b[0m(\u001b[96mself\u001b[0m, optimizer, optimizer_state, *args, **kwargs):                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2m│   │   \u001b[0mretval = \u001b[94mNone\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m284 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96msum\u001b[0m(v.item() \u001b[94mfor\u001b[0m v \u001b[95min\u001b[0m optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m].values()):    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m285 \u001b[2m│   │   │   \u001b[0mretval = optimizer.step(*args, **kwargs)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m retval                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mstep\u001b[0m(\u001b[96mself\u001b[0m, optimizer, *args, **kwargs):                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33mlr_scheduler.py\u001b[0m:\u001b[94m65\u001b[0m in \u001b[92mwrapper\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  62 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minstance = instance_ref()                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  63 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minstance._step_count += \u001b[94m1\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  64 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mwrapped = func.\u001b[92m__get__\u001b[0m(instance, \u001b[96mcls\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  65 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapped(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  66 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  67 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Note that the returned function here is no longer a bound method,\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  68 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# so attributes like `__func__` and `__self__` no longer exist.\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m113\u001b[0m in \u001b[92mwrapper\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mobj, *_ = args                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mprofile_name = \u001b[33m\"\u001b[0m\u001b[33mOptimizer.step#\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m.step\u001b[0m\u001b[33m\"\u001b[0m.format(obj.\u001b[91m__class__\u001b[0m.\u001b[91m__name__\u001b[0m)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.autograd.profiler.record_function(profile_name):                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m113 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0mhooked = \u001b[96mgetattr\u001b[0m(\u001b[96mself\u001b[0m.\u001b[91m__class__\u001b[0m.step, \u001b[33m\"\u001b[0m\u001b[33mhooked\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33moptimization.py\u001b[0m:\u001b[94m436\u001b[0m in \u001b[92mstep\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m433 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Exponential moving average of gradient values\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m434 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mexp_avg\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros_like(p.data)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m435 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Exponential moving average of squared gradient values\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m436 \u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mexp_avg_sq\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros_like(p.data)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m437 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m438 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mexp_avg, exp_avg_sq = state[\u001b[33m\"\u001b[0m\u001b[33mexp_avg\u001b[0m\u001b[33m\"\u001b[0m], state[\u001b[33m\"\u001b[0m\u001b[33mexp_avg_sq\u001b[0m\u001b[33m\"\u001b[0m]                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m439 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mbeta1, beta2 = group[\u001b[33m\"\u001b[0m\u001b[33mbetas\u001b[0m\u001b[33m\"\u001b[0m]                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m58.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.76\u001b[0m GiB total capacity; \u001b[1;36m13.24\u001b[0m GiB already \n",
       "allocated; \u001b[1;36m19.75\u001b[0m MiB free; \u001b[1;36m13.93\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('/aiffel/aiffel/NLP/KoChatGPT/output_1_SFT_quest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model='/aiffel/aiffel/NLP/KoChatGPT/output_1_SFT_quest', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e706ecb1",
   "metadata": {},
   "source": [
    "### RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6ada1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a640b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb55b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=1, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/aiffel/aiffel/NLP/KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1738ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f8c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed1e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547324c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31991f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained('/aiffel/aiffel/NLP/KoChatGPT/output_2_RM_quest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51822f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeead0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 메모리 관리를 위한 캐시 비우기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41bea24",
   "metadata": {},
   "source": [
    "### Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbeef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9ff340",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='/aiffel/aiffel/NLP/KoChatGPT/output_1_SFT_quest', lora_rank=0)\n",
    "    critic = GPTActor(pretrained='/aiffel/aiffel/NLP/KoChatGPT/output_2_RM_quest', lora_rank=0)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    pdding_side='right',\n",
    "    model_max_length=512\n",
    "    )\n",
    "    \n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3c8beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f69b61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355cd1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/aiffel/KoChatGPT/data_kochatgpt/kochatgpt/kochathpt_3_PPO.jsonl', 'r', encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "    \n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.itmes()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d6e1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize_fn('It takes something more than intelligence to act intelligently.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3b6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(NaiverStrategy(),\n",
    "                    actor,\n",
    "                    critic,\n",
    "                    reward_model,\n",
    "                    initial_model,\n",
    "                    actor_optim,\n",
    "                    critic_optim,\n",
    "                    max_epochs=1,\n",
    "                    train_batch_size=8,\n",
    "                    tokenizer=tokenize_fn,\n",
    "                    max_length=128,\n",
    "                    do_sample=True,\n",
    "                    temperatur=1.0,\n",
    "                    top_k=50,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380b68de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(list_prompt,\n",
    "           num_episodes=10,\n",
    "           max_timestpes=3,\n",
    "           update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('/aiffel/aiffel/NLP/KoChatGPT/output_3_PPO_quest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7624288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf6d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba861d95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb382287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26ce9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb17f66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebdb62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ceddab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1bef09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a58ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1696a7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4d81f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd87991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f27643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71245d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bleu\n",
    "\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# 모델이 생성한 번역 결과와 참조 번역 결과\n",
    "candidate_sentences = [\n",
    "    ['I', 'am', 'going', 'to', 'the', 'park', 'today', '.'],\n",
    "    ['The', 'cat', 'is', 'on', 'the', 'mat', '.']\n",
    "]\n",
    "\n",
    "reference_sentences = [\n",
    "    [['I', 'am', 'going', 'to', 'the', 'park', 'today', '.'],\n",
    "     ['I', 'am', 'going', 'to', 'the', 'park', 'today', '.'],\n",
    "     ['I', 'am', 'going', 'to', 'the', 'park', 'today', '.'],\n",
    "     ['I', 'am', 'going', 'to', 'the', 'park', 'today', '.']],\n",
    "    [['The', 'cat', 'is', 'on', 'the', 'mat', '.'],\n",
    "     ['The', 'cat', 'is', 'sitting', 'on', 'the', 'mat', '.']]\n",
    "]\n",
    "\n",
    "# BLEU 계산\n",
    "bleu_score = corpus_bleu(reference_sentences, candidate_sentences)\n",
    "\n",
    "print(f\"BLEU Score: {bleu_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284f5d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge\n",
    "from rouge import Rouge\n",
    "\n",
    "# 모델이 생성한 요약 결과와 참조 요약 결과\n",
    "candidate_summary = \"The cat is on the mat.\"\n",
    "reference_summary = \"The cat is sitting on the mat.\"\n",
    "\n",
    "# Rouge 계산\n",
    "rouge = Rouge()\n",
    "scores = rouge.get_scores(candidate_summary, reference_summary)\n",
    "\n",
    "print(\"ROUGE-N: \", scores[0]['rouge-1'])\n",
    "print(\"ROUGE-L: \", scores[0]['rouge-l'])\n",
    "print(\"ROUGE-W: \", scores[0]['rouge-w'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39465c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
